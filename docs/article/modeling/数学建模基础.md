---
title: "数学建模基础"
shortTitle: "建模基础"
description: "记录数学建模必要的基础知识"
icon: ""
author: 
  name: gzw
  url: https://www.gzw-icu.com
  email: 1627121193@qq.com
isOriginal: false
date: 2023-01-20
category: 
- "数学建模"
- "算法"
tag:
- "数学建模"
- "算法"
sticky: 1
star: false
article: true
timeline: true,
dir:
  text: 数学建模基础
  icon: ""
  collapsible: true
  index: true
  comment: true
headerDepth: 3
index: true
order: 2
copy:
  triggerWords: 100
  disableCopy: false
  disableSelection: false
feed:
  title: "数学建模基础"
  description: "记录数学建模必要的基础知识"
  author:
    name: gzw
    email: 1627121193@qq.com
---







# 数学建模基础

三大块：

- 建模
- 编程
- 协作





# 模型

## 层次分析法

解决评价类问题

可以用打分解决，定指标，指标需要：权重/重要性

指标如何选择？

- 搜索

- 题目提炼
- 文献

流程：

确定指标 -> 设定权重 -> 进行分析 -> 评价得分

两两比较确定权重，需要使用矩阵：

<img src="https://my-photos-1.oss-cn-hangzhou.aliyuncs.com/markdown/%E5%BB%BA%E6%A8%A1/20230206/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95%E6%AF%94%E8%BE%83%E7%9F%A9%E9%98%B5.png" alt="image-20230206220200655" style="zoom:50%;" />

**一致矩阵：**

<img src="https://my-photos-1.oss-cn-hangzhou.aliyuncs.com/markdown/%E5%BB%BA%E6%A8%A1/20230206/%E4%B8%80%E8%87%B4%E7%9F%A9%E9%98%B5.png" alt="image-20230206222800917" style="zoom:50%;" />

权重求法：

1. 算数平均法求权重
2. 几何平均法求权重
3. 特征值法求权重

三个问题：

1. 评价目标是什么
2. 选择什么方案
3. 评价的准则是什么

注意：层次结构图必须要放到论文中（可用亿图展示画图）

局限性：

1. 评价的决策层不能太多，否则判断矩阵和一致性矩阵差异可能会非常大
2. 如果决策层中的指标的数据已知，那么我们一般就不可以再用层次分析法





## TOPSIS 法

逼近理想解排序法/优劣解距离法

指标：

- 越大越好，极大型指标（效益型指标）
- 越小越好，极小型指标（成本型指标）
- 中间型指标，越接近某个值越好
- 区间型指标，落在某个区间最好

将所有的指标转换为极大型为：指标正向化（最常用）

极小型指标转换成极大型指标的公式：max - x

正向化后的指标一般都要进行「标准化处理」，以此消除量纲，这样不同指标之间才能求和

所谓的将原式矩阵正向化，即使将所有的指标类型统一转换为极大型指标

基本流程：

矩阵正向化 -> 标准化 -> 计算得分并归一化

改进：

基于「熵权法」对该模型的修正是一种较为客观的赋权方法

熵权法依据：

指标的的变异程度越小，所反映的信息量越小，其对应的权值就越小（数据本身就可以告诉我们权重）





## 插值算法

比赛中的数据常常缺少某些值，需要模拟补全缺少的值

插值分类：

1. 分段插值（最常用）（可以避免龙格现象，分段低次插值：线性插值、二次插值）
2. 插值多项式（拉格朗日插值法，有可能出现龙格现象）
3. 三角插值（主要使用到傅里叶变换）

具体的插值法：

- 牛顿插值法
- 埃尔米特插值法（常用）（导数也相似，需要分段，往往使用分段三次插值：PCHIP）
- 三次样条插值（常用）
- 一维插值和 N 维插值



## 拟合算法

最终曲线不用经过每一个点，只需要非常接近即可，这样曲线的函数会简单（插值法生成的曲线往往都会有很复杂的函数）

具体的拟合算法：

- 最小二乘法

评价拟合的好坏：拟合优度：R^2

- 总体平方和：SST
- 误差平方和：SSE
- 回归平方和：SSR

SST = SSE + SSR

R^2 = SRR / SST = 1 - SSE / SST，R^2 越接近 1，说明误差平方和越接近 0，误差越小说明拟合越好

但是注意：R^2 只能用于的拟合函数是「线性函数」（线性是指参数线性）

MATLAB 自带了曲线拟合工具箱

注意：插值或者拟合都可以用于「预测」





## 相关系数

两种常用的相关系数：

1. 皮尔逊（person）相关系数
2. 斯皮尔曼（spearman）等级相关系数

要区分总体和样本



协方差的大小和两个变量的量纲有关，不适合做比较

皮尔逊相关系数也可以看作是剔除了两个变量量纲影响，即将 X 和 Y 标准化后的协方差（协方差除于两个变量的标准差）

补充：还分为了总体皮尔逊相关系数和样本皮尔逊相关系数



相关性系数的意义：

相关系数的绝对值越接近于 1，越接近一条线



### 皮尔逊相关系数

皮尔逊相关系数的误区：

数据中可能有异常值，最后会导致错误的相关系数

皮尔逊相关系数受异常值的影响非常大

相关系数知识用来衡量两个变量线性相关程度的指标

也就是说：必须先确认这两个变量是线性相关的，然后这个相关系数才能告诉你两个变量的相关程度如何



容易忽视的点：

1. 非线性相关也会导致线性相关系数很大
2. 离群点对相关系数的影响也很大
3. 如果两个变量的相关系数很大也不能说明两者相关，可能是受到了异常值的影响
4. 相关系数计算结果为 0，只能说不是线性相关，但是说不定会有更复杂的相关关系



总结：

1. 如果两个变量本身就是线性关系，那么皮尔逊相关系数绝对值大的就是相关性强，小的就是相关性弱
2. 在不确定两个变量是说明关系的情况下，即使算出皮尔逊相关系数很大，也不能说明那两个变量线性相关，甚至不能说他们相关，一定要画出散点图才能看出来（有可能有异常值）



对相关系数的解释是依赖于具体的应用背景和目的的

事实上，比起相关系数的大小，我们往往更关注的是显著性（假设检验）



补充：描述性统计：一般就是统计数据中的平均值、标准误差、中位数、众数、标准差、方差、峰值、偏度、区域、最小最大值、求和、观测值等

皮尔逊相关系数需要进行假设检验以及计算 P 值

皮尔逊相关系数假设检验的条件：

1. 数据通常假设是成对的来自于正态分布的总体
2. 数据之间的差距不能太大
3. 每组样本之间都是独立抽样的 



怎么检验数据是否为正态分布：

1. 正态分布 JB 检验（跟偏度和峰度有关），样本越大效果越好（N > 50）
2. Shapiro-wilk 夏皮洛-威尔克检验（3 < N < 50），当样本较少时使用
3. Q-Q 图（Q 代表分位数）：通过比较两个概率分布的分位数对这两个概率进行比较的概率图方法；要利用 Q-Q 鉴别样本数据是否近似于正态分布，只需要看图上的点是否近似地在一条直线附近（大样本时使用）



### 斯皮尔曼相关系数

对数据的要求比较低

分为小样本和大样本两种情况

小样本情况下（N < 30 时），直接查表，样本的相关系数必须大于等于表（自己搜）中的临界值，才能得出显著的结论

大样本下，需要计算检验值，之后求出对应的 P 值与 0.5 比较即可



### 相关系数总结

<img src="https://my-photos-1.oss-cn-hangzhou.aliyuncs.com/markdown//%E5%BB%BA%E6%A8%A1/20230207/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E6%AF%94%E8%BE%83.png" alt="image-20230207223510635" style="zoom:50%; float: left" />





## 假设检验

分为单侧检验和双侧检验

最常用的置信水平是 95%

步骤：

1. 确定原定假设和备择假设
2. 根据要检验的量构造一个分布（这一步较难）
3. 画出这个分布的概率密度图





## 典型相关分析

研究两组变量（魅族变量中都可能有多个指标）之间相关关系的一种多元统计方法，能够揭示出两组变量之间的内在联系

基本思想和主成分分析非常相似（后者用于降维）：

<img src="https://my-photos-1.oss-cn-hangzhou.aliyuncs.com/markdown//%E5%BB%BA%E6%A8%A1/20230207/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E7%9A%84%E5%AE%9A%E4%B9%89.png" alt="image-20230207224347160" style="zoom:50%; float: left" />

样本典型相关变量分析：

1. 计算样本典型相关变量以及典型相关系数
2. 相关系数的显著性检验

典型相关性分析中的问题：

1. 相关矩阵出发计算典型相关（先标准化消除量纲的影响）
2. 典型载荷分析（原始变量与典型变量之间的相关性分析）
3. 典型冗余分析（定量测度典型变量锁包含的原始信息的大小，这个使用的较少）

补充：这里如果使用 SPSS 进行分析，会给出上述所有相关的结果（需要有 Python 环境）







## 多元线性回归分析

相关性不是因果性

因变量类型：

1. 连续数值型变量：对应线性回归，模型有：OLS、GLS
2. 0-1 型变量：对应 0-1 回归，模型有：logistic 回归
3. 定序变量：对应定序回归，模型有：probit 定序回归
4. 计数变量：对应计数回归，模型有泊松回归
5. 生存变量（截断数据）：对应生存回归，模型有： Cox 等比例风险回归

回归分析的任务就是：通过研究 X 和 Y 的相关关系，尝试去解释 Y 的形成机制，进而达到通过 X 去预测 Y 的目的

回归分析的三个重要作用：

1. 识别重要变量
2. 判断相关性的方向
3. 估计权重（回归系数）



数据分类：

<img src="https://my-photos-1.oss-cn-hangzhou.aliyuncs.com/markdown//%E5%BB%BA%E6%A8%A1/20230208/%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png" alt="image-20230208143846648" style="zoom:50%;" />

横截面数据：在某一时间点收集的不同对象的数据

时间序列数据：对同一对象在不同时间连续观察所取得的数据

面板数据：横截面数据与时间序列数据综合起来的一种数据资源





### 一元线性回归

线性假定并不要求初始模型都呈严格线性关系，因为自变量和因变量都可通过变量转换成线性模型

使用线性回归模型建模前，需要对数据进行预处理，可以用 Excel、Stata 等

变量导致的内生性：如果满足误差项和所有的自变量 x 都不相关，那么称该回归模型具有外生性，如果相关就存在内生性，内生性会导致回归系数估计的不准确：不满足无偏和一致性

蒙特卡罗模拟可以考察内生性的大小

要求所有的解释变量（自变量）全部外生很困难，可以弱化这个条件

核心解释变量时我们感兴趣的变量，我们特别希望得到对其系数的一致估计（当样本容量无限大时，收敛于待估计参数的真值）

控制变量：对这些变量没太大兴趣

在实际应用中，主要保证核心解释变量与误差项不相关即可





四种模型：

1. 一元线性回归
2. 双对数模型
3. 半对数模型（两种）



步骤：

1. 描述性数据统计
2. 计算拟合优度
3. 标准化回归系数（去除量纲影响，标准化系数的绝对值越大，说明对因变量的影响就越大（只关注显著的回归系数））



扰动项：

1. 球型扰动项：满足同方差和无自相关两个条件（横截面数据容易出现异方差问题，时间序列数容易出现自相关问题）



如果扰动项存在异方差：

1. OLS 估计出来的回归系数是无偏、一致的
2. 假设检验无法使用
3. OLS 估计量不再是最优线性无偏估计量

怎么解决异方差：

1. 使用 OLS + 稳健的标准误差
2. 广义的最小二乘法

原理：方差较小的数据包含的信息较多，可以给予信息量大的数据更大的权重（即方差小的数据给予更大的权重）



如何检验异方差：

1. 画出残差与拟合值的散点图
2. 画残差与自变量的散点图
3. BP 检验
4. 怀特检验



多重共线性：如果某一解释变量可以由其他的解释变量线性表出，则存在严格多重共线性，这会使得对系数的估计变得不准确

如何处理多重共线性：

1. 如果不关心具体的回归系数，只关心整个方程预测被解释变量的能力，可以不必理会多重共线性（假设整个方程是显著的）
2. 如果关心具体的回归系数，但多重共线性并不影响所关心变量的显著性，那么也可以不必理会
3. 如果多重共线性影响到所关心变量的显著性，则需要增大样本容量，剔除导致严重共线性的变量（但是不要轻易剔除，因为可能会有内生性的影响），或者是对模型进行修改



逐步回归分析：

可以解决共线性

- 先前逐步回归
- 向后逐步回归





## 图的最短距离

图论基础：

无向图的权重邻接矩阵：

1. 无向图对应的权重邻接矩阵是一个对称矩阵（有向图一般不是对称的），其对角线上的元素为 0
2. $D_ij$  表示第 i 个点到第 j 个节点的权重



两个算法：

1. 迪杰斯特拉算法（可以用于有向图，但是不能处理负权重）
2. 弗洛伊德算法（不支持含有负权重回路的图）
3. 贝尔曼-福特算法（不支持含有负权重回路的图，但是可以处理具有负权重的「有向图」）



什么是负权回路：

1. 在一个图中每条边都有一个权重（有正有负）
2. 如果存在一个环，从某点出发最后又回到自己，而且环上的「所有权值和为负数」，那么就称为负权回路
3. 存在负权回路的图不能求两点之间最短路径，因为只要在负权回路上不停绕圈，所得的最短长度可以任意小





## 分类模型

> 回归分为解释型回归与预测型回归

通过数据判断类型，分类二分类和多分类

使用逻辑回归：

- 线性概率模型（LPM）
- 两点分布

- 非线性模型，可以使用极大似然估计法（MLE）进行估计



预测型回归，加入平方项后可能会出现过拟合现象，这是对于样本数据的预测会非常好，但是对于样本外的数据的预测效果可能会很差（与龙格现象有点相似）

如何确定合适的模型：把数据分为「训练组」和「测试组」，用训练组的数据来估计出模型，再用测试组的数据来进行测试（一般比例是 8 : 2）



Fisher 线性判别分析：

该方法的思想：给定训练集样例，设法将样例投影到一维的直线上，使得同样样例的投影点尽可能接近和密集，异类投影点尽可能远离

核心问题就是：找到线性向量







## 聚类模型

将样本划分为由类的对象组成的多个类的过程

聚类后可以更加准确的在每个类中单独使用统计模型进行估计、分析或者预测，也可以探究不同类之间的相关性和主要差异

分类和聚类的分别：分类是已知类别，聚类是未知类别



### K-means 聚类算法

1. 确定类的个数 K，选定中心
2. 更新迭代中心直到不再变化

优点：

1. 算法简单、快速
2. 对处理大数据集效率高

缺点：

1. 要求实现给定生成的类型数 K
2. 对初值敏感
3. 对孤立点数据敏感

K-means++ 可以解决上述 2、3 个缺点



### 系统（层次）聚类

聚类一般是对样本聚类，很少对指标聚类

需要确定样本之间的常用距离

- 绝对值距离
- 欧氏距离
- Minkowski 距离
- Chebyshev 距离
- 马氏距离

确定不同类的距离：

- 重心法求解距离

- 最短距离法
- 组内连接平均法

步骤：

1. 将每个对象看作一类，计算两两之间的最小距离
2. 将距离最小的两个类合并成一个新类
3. 重新计算心累与所有类之间的距离
4. 重复 2、3 步骤，直到所有类最终合并成一类

聚类分析需要注意的问题：

1. 要根据分类的目的选取指标，指标的选取不同，分类的结果一般也不一样
2. 样本间距离定义的方式以及聚类的方法不同，聚类的结果一般也不一样
3. 要注意指标的量纲，量纲差别太大会导致聚类结果不合理
4. 聚类分析的结果可能不令人满意，需要对结果找到一个合理的解释

聚类数量的估计：

1. 肘部法则：通过图形大致估计出聚类数量



### DBSCAN 算法

基于密度的聚类算法，不需要预先指定聚类的个数，要求一定区域的对象不小于某一个阈值

该方法能在具有噪声的数据空间数据库中发现任意形状的簇

基本概念：

该算法将数据分为三类：

1. 核心点：在半径内含有不少于阈值数目的点
2. 边界点：在半径内点的数量小于阈值，但是罗战核心点的邻阈内
3. 噪声点：既不是核心点，也不是边界点

优点：

1. 基于密度定义，能够处理任意形状和大小的簇
2. 可在聚类的同时发现异常点
3. 与 K-means 算法比较起来，不需要输入要划分的聚类的个数

缺点：

1. 对输入的参数和阈值比较敏感，确定参数困难
2. 由于该算法的参数和阈值是全局唯一的，当聚类的密度不均匀时，聚类距离相差很大时，计算密度单元的计算复杂度大
3. 当数据量大时，计算密度单元的计算复杂度大

建议：

1. 只有两个，且在散点图做出来后看起来有一定的形状时，可以使用该算法进行聚类
2. 其他情况下一般都使用系统聚类





## 时间序列分析

两个要素：

1. 时间要素（分为时点时间序列和时期时间序列）
2. 数值要素

不同趋势：

1. 长期趋势
2. 季节趋势
3. 循环变动趋势（若干年为趋势）

不同模型：

1. 叠加模型（变动互不影响）
2. 乘积模型（变动相互影响）



替换缺失值的方法：

1. 序列平均值
2. 邻近点平均值
3. 邻近点中位数
4. 线性插值
5. 邻近点的线性趋势

时间序列分析用于：

1. 描述过去
2. 分析规律
3. 预测未来

分析的步骤：

1. 做时间序列图
2. 判断时间序列包含的变动成分
3. 时间序列分解
4. 建立时间序列分析模型
5. 预测未来的指标数值



时间序列模型：

1. 指数平滑法模型：
   1. simple 模型（只能预测一期）
   2. Holt 线性趋势（适用于线性趋势，不含季节成分）
   3. Brown 线性趋势（适用于线性趋势，不含季节成分）
   4. 阻尼趋势（适用于线性趋势逐渐减弱且不含季节成分）
   5. 简单季节性（含有稳定的季节成分，不含趋势）
   6. Winters 可加性（含有线性趋势和稳定的季节成分）
   7. Winters 可乘性（含有线性趋势和不稳定季节成分）
2. ARIMA 模型：
   1. ARIMA 模型
   2. SARIMA 模型



ARIMA 模型相关概念：

1. 平稳时间序列和白噪声序列
2. 差分方程和滞后算子
3. AR 模型
4. MA 模型
5. ARMA 模型
6. ACF 和 PACF
7. ARMA 模型的估计
8. AIC 和 BIC 准则



## 预测模型

具有完全多重共线性是不可能求出回归方程的



### 灰度预测模型

灰色预测模型：初衷时对数列建立近似的微分方程模型，但是这只适用于连续可微方程，而事件序列数据是非连续的，所以灰色预测模型得到的只是近似微分方程

GM(1, 1)：

在使用 G(1, 1) 对未来的数据进行预测时，需要先检验该模型对原数据的拟合程度，一般有两种检验方法：

- 残差检验
- 级比偏差检验

灰色预测的使用场景：

1. 数据是以年份度量的非负数据（如果是月份或者季度数据就一定要用时间序列模型）
2. 数据能经过准指数规律的检验（除了前两期外，后面至少 90% 的期数的光滑比要低于 0.5）
3. 数据的期数较短且和其他数据之间的关联性不强（小于等于 10，但是也不能太短，要是数据期数较长，一般使用传统的事件序列模型）

预测时：

1. 要结果背景、合理假设
2. 不要硬套模型、不做解释

面对预测题目时：

1. 先画出时间序列图并简单分析趋势
2. 将数据分为训练组和实验组，尝试使用不同的模型对训练组进行建模，并利用实验组的数据判断哪种模型的预测效果最好（比如可以使用 SSE 这个指标来挑选模型，常见的模型有指数平滑、ARIMA、灰度预测、神经网络）
3. 选择预测误差最小的模型，并利用全部数据重新建模，并对未来的数据进行预测
4. 画出预测后的数据和原数据的序列图，查看预测的未来趋势是否合理





### 神经网络

机器学习的数据分类：

- 训练集
- 验证集
- 测试集

注意：神经网络很大几率过拟合，要有验证集和测试机

BP 神经网络预测（万金油）

有三种训练方法：

1. 莱文贝格-马夸特方法（梯度下降）
2. 贝叶斯正则化方法
3. 量化共轭梯度法





## SVD 和图形处理

SVD：奇异值分解（矩阵的分解），可以将数据降维，将图形压缩

 这里说的降维，更准确的是说使得矩阵的秩减小，矩阵的大小并没有减小

优点：可以简化数据，去除噪声点，对数据进行降维

缺点：数据的转换可能难以理解

适用的数据类型：数值型

通过 SVD 对数据进行处理，我们可以对原始数据进行精简，这样做实际上是去除了噪声和冗余信息，以此达到优化数据的目的

SVD 的其他重要应用：

1. 潜在语义索引（可以阅读吴军老师的《数学之美》）
2. 推荐系统





## 主成分分析

主成分分析是一种降维算法，能将多个指标转换为少数几个主成分，这些主成分是原始变量的线性组合，彼此之间互不相关，其能反映出原始数据的大部分信息

当研究的问题涉及到多变量且变量之间存在很强的相关性时，我们可以考虑使用主成分分析来对数据进行简化

降维具有的优点：

1. 使得数据集更易使用
2. 降低算法的计算开销
3. 去除噪声
4. 使得结果更易理解

主成分的解释其含义一般带有模糊性，不像原始变量的含义那么清楚，这是变量降维过程中不得不付出的代价

主成分分析的困难之处在于要能给出主成分的较好解释，所以提取的主成分中如果有一个解释不了，那么主成分分析就是失败了

主成分分析可以用于聚类































































































# 补充

## 数据收集站点

[简道云汇总](https://tyi45di4ct.jiandaoyun.com/f/5e7d7262d70fb900060607c1)

[虫部落](https://search.chongbuluo.com/)

[大数据导航](http://hao.199it.com/)

[数据平台](http://hippter.com/data.html)





